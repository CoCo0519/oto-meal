#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart post-processor for files generated by an external program (e.g., BLE_CENTRAL.exe).
- Watches a directory for new .txt files (Watchdog if available; otherwise simple polling).
- Waits until files are fully written (size stabilizes).
- Extracts metadata (timestamp/device/fs/kind if present) and renames with a configurable pattern.
- Optionally modifies content (normalize newlines, ensure UTF-8, prepend header, create JSON sidecar).
- Can backfill existing files and optionally launch the target EXE.

Usage (basic):
  python smart_postprocess.py --watch-dir "D:\Data\BLE" --exe "D:\Tools\BLE_CENTRAL.exe"
...
"""
import argparse
import datetime as dt
import json
import logging
import os
import re
import sys
import time
from pathlib import Path

USE_WATCHDOG = False
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    USE_WATCHDOG = True
except Exception:
    USE_WATCHDOG = False

DEFAULT_CONFIG = {
    "watch_dir": ".",
    "extensions": [".txt", ".log"],
    "pattern": "{timestamp:%Y%m%d_%H%M%S}-{device}-{kind}-{seq:03d}.txt",
    "move_to_subdir_per_day": True,
    "per_day_folder_format": "%Y-%m-%d",
    "modify": {
        "normalize_newlines": "crlf",
        "ensure_utf8": True,
        "add_header": True,
        "header_prefix": "# ",
        "create_json_sidecar": True
    },
    "backfill_existing": True,
    "poll_seconds": 1.0,
    "stable_checks": 3,
    "launch_exe": None,
    "args_for_exe": [],
    "log_path": "smart_postprocess.log"
}

INVALID_WIN_CHARS_RE = re.compile(r'[<>:"/\\|?*\x00-\x1F]')
DEVICE_RE = re.compile(r'(?i)(?:device|mac)\s*[:=]\s*([0-9A-F]{2}(?:[:\-][0-9A-F]{2}){5})')
FS_RE     = re.compile(r'(?i)\bfs\s*[:=]?\s*(\d{1,5})\s*hz\b')
KIND_RE   = re.compile(r'(?i)\b(swallow|cough|chew|speak)\b')
TIME_RE   = re.compile(r'(?i)\b(20\d{2}[-/\.]\d{1,2}[-/\.]\d{1,2}[ T_]\d{1,2}:\d{2}:\d{2})\b')

def sanitize_filename(name: str, replacement: str = "-") -> str:
    name = INVALID_WIN_CHARS_RE.sub(replacement, name)
    while replacement*2 in name:
        name = name.replace(replacement*2, replacement)
    return name.strip(" .") or "unnamed"

def detect_encoding_and_read(path: Path) -> str:
    for enc in ("utf-8", "utf-8-sig", "gb18030", "latin1"):
        try:
            with open(path, "r", encoding=enc, errors="strict") as f:
                return f.read()
        except Exception:
            continue
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        return f.read()

def normalize_newlines(text: str, mode: str | None) -> str:
    if not mode:
        return text
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    if mode.lower() == "lf":
        return text
    if mode.lower() == "crlf":
        return text.replace("\n", "\r\n")
    return text

def wait_for_stable(path: Path, stable_checks: int, sleep_s: float) -> bool:
    prev = -1
    stable = 0
    while True:
        if not path.exists():
            return False
        try:
            size = path.stat().st_size
        except Exception:
            size = -1
        if size == prev and size >= 0:
            stable += 1
        else:
            stable = 0
        prev = size
        if stable >= stable_checks:
            return True
        time.sleep(sleep_s)

def parse_metadata(text: str, fallback_mtime: float) -> dict:
    md = {}
    dev = DEVICE_RE.search(text)
    fs  = FS_RE.search(text)
    kind= KIND_RE.search(text)
    tmatch = TIME_RE.search(text)
    md["device"] = dev.group(1) if dev else None
    md["fs"] = fs.group(1) if fs else None
    md["kind"] = kind.group(1).lower() if kind else "raw"
    if tmatch:
        s = tmatch.group(1).replace("_", " ").replace(".", "-").replace("/", "-")
        try:
            md["timestamp"] = dt.datetime.fromisoformat(s)
        except Exception:
            md["timestamp"] = dt.datetime.fromtimestamp(fallback_mtime)
    else:
        md["timestamp"] = dt.datetime.fromtimestamp(fallback_mtime)
    return md

def daily_sequence(dest_dir: Path, day_str: str) -> int:
    seq = 1
    try:
        for p in dest_dir.glob("*"):
            if not p.is_file():
                continue
            if day_str in p.name:
                seq += 1
    except Exception:
        pass
    return seq

def build_new_name(pattern: str, md: dict, original_name_noext: str, dest_dir: Path) -> str:
    ts = md.get("timestamp") or dt.datetime.now()
    tokens = {
        "timestamp": ts,
        "date": ts.strftime("%Y%m%d"),
        "time": ts.strftime("%H%M%S"),
        "device": (md.get("device") or "NODEV").replace(":", "-"),
        "fs": md.get("fs") or "NA",
        "kind": md.get("kind") or "raw",
        "basename": original_name_noext,
    }
    day_str = tokens["date"]
    tokens["seq"] = daily_sequence(dest_dir, day_str)

    class FallbackDict(dict):
        def __missing__(self, key):
            return "NA"

    formatted = pattern.format_map(FallbackDict(tokens))
    return sanitize_filename(formatted)
def write_modified(path: Path, text: str, cfg: dict, md: dict, original: Path):
    mcfg = cfg.get("modify", {})
    text = normalize_newlines(text, mcfg.get("normalize_newlines"))
    enc = "utf-8" if mcfg.get("ensure_utf8", True) else "utf-8"

    header_lines = []
    if mcfg.get("add_header", True):
        prefix = mcfg.get("header_prefix", "# ")
        ts = (md.get("timestamp") or __import__("datetime").datetime.now()).strftime("%Y-%m-%d %H:%M:%S")
        header = {
            "source_file": str(original.name),
            "timestamp": ts,
            "device": md.get("device") or "",
            "fs_hz": md.get("fs") or "",
            "kind": md.get("kind") or "",
        }
        header_lines = [f'{prefix}{k}: {v}' for k, v in header.items() if v != ""]
        if not any(l.startswith(prefix) for l in text.splitlines()[:5]):
            text = "\n".join(header_lines) + ("\n\n" if text and not text.startswith("\n") else "\n") + text

    with open(path, "w", encoding=enc, errors="replace", newline="") as f:
        f.write(text)

    if mcfg.get("create_json_sidecar", True):
        sidecar = path.with_suffix(".json")
        payload = {
            "renamed_to": path.name,
            "source_file": str(original.name),
            "metadata": {
                "timestamp": (md.get("timestamp") or __import__("datetime").datetime.now()).isoformat(),
                "device": md.get("device"),
                "fs_hz": md.get("fs"),
                "kind": md.get("kind"),
            }
        }
        try:
            with open(sidecar, "w", encoding="utf-8") as jf:
                json.dump(payload, jf, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.exception("Failed to write sidecar JSON: %s", e)

def process_one_file(path: Path, cfg: dict) -> Path | None:
    if not path.exists() or not path.is_file():
        return None
    exts = set(x.lower() for x in cfg.get("extensions", [".txt"]))
    if path.suffix.lower() not in exts:
        return None

    logging.info("Detected new file: %s", path)
    if not wait_for_stable(path, cfg.get("stable_checks", 3), cfg.get("poll_seconds", 1.0)):
        logging.warning("File disappeared before processing: %s", path)
        return None

    text = detect_encoding_and_read(path)
    stat = path.stat()
    md = parse_metadata(text, fallback_mtime=stat.st_mtime)

    dest_dir = Path(cfg["watch_dir"])
    if cfg.get("move_to_subdir_per_day", True):
        per_fmt = cfg.get("per_day_folder_format", "%Y-%m-%d")
        day_dir = md["timestamp"].strftime(per_fmt)
        dest_dir = dest_dir / day_dir
        dest_dir.mkdir(parents=True, exist_ok=True)
    else:
        dest_dir.mkdir(parents=True, exist_ok=True)

    new_name = build_new_name(cfg.get("pattern", "{timestamp:%Y%m%d_%H%M%S}-{device}-{kind}-{seq:03d}.txt"), md, path.stem, dest_dir)
    dest_path = dest_dir / new_name

    idx = 1
    stem, suf = os.path.splitext(dest_path.name)
    while dest_path.exists():
        dest_path = dest_dir / f"{stem}__{idx}{suf}"
        idx += 1

    try:
        write_modified(dest_path, text, cfg, md, original=path)
        bak = path.with_suffix(path.suffix + ".bak")
        try:
            os.replace(path, bak)
        except Exception:
            try:
                path.unlink(missing_ok=True)
            except Exception:
                pass
        logging.info("Processed -> %s", dest_path)
        return dest_path
    except Exception as e:
        logging.exception("Failed processing %s: %s", path, e)
        return None

def backfill_existing(cfg: dict):
    wd = Path(cfg["watch_dir"])
    if not wd.exists():
        return
    for ext in cfg.get("extensions", [".txt"]):
        for p in wd.glob(f"*{ext}"):
            if p.suffix.endswith(".bak"):
                continue
            process_one_file(p, cfg)
def run_watchdog(cfg: dict):
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler

    class Handler(FileSystemEventHandler):
        def __init__(self, cfg): self.cfg = cfg
        def on_created(self, event):
            if event.is_directory: return
            try: process_one_file(Path(event.src_path), self.cfg)
            except Exception as e: logging.exception("on_created failed: %s", e)

    wd = Path(cfg["watch_dir"]).resolve()
    event_handler = Handler(cfg)
    observer = Observer()
    observer.schedule(event_handler, str(wd), recursive=False)
    observer.start()
    try:
        while True:
            time.sleep(1.0)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

def run_polling(cfg: dict):
    wd = Path(cfg["watch_dir"]).resolve()
    seen = set()
    try:
        while True:
            for ext in cfg.get("extensions", [".txt"]):
                for p in wd.glob(f"*{ext}"):
                    if p in seen:
                        continue
                    process_one_file(p, cfg)
                    seen.add(p)
            time.sleep(cfg.get("poll_seconds", 1.0))
    except KeyboardInterrupt:
        return

def maybe_launch_exe(cfg: dict):
    exe = cfg.get("launch_exe")
    if not exe:
        return None
    try:
        import subprocess
        args = [exe] + list(cfg.get("args_for_exe", []))
        logging.info("Launching EXE: %s", " ".join(args))
        subprocess.Popen(args, shell=False)
    except Exception as e:
        logging.exception("Failed to launch EXE: %s", e)

def load_config(args_ns) -> dict:
    cfg = json.loads(json.dumps(DEFAULT_CONFIG))
    if args_ns.config and Path(args_ns.config).exists():
        with open(args_ns.config, "r", encoding="utf-8") as f:
            user_cfg = json.load(f)
        def deep_merge(a, b):
            for k, v in b.items():
                if isinstance(v, dict) and isinstance(a.get(k), dict):
                    deep_merge(a[k], v)
                else:
                    a[k] = v
        deep_merge(cfg, user_cfg)
    if args_ns.watch_dir:
        cfg["watch_dir"] = args_ns.watch_dir
    if args_ns.pattern:
        cfg["pattern"] = args_ns.pattern
    if args_ns.launch_exe:
        cfg["launch_exe"] = args_ns.launch_exe
    if args_ns.extensions:
        cfg["extensions"] = args_ns.extensions
    if args_ns.backfill is not None:
        cfg["backfill_existing"] = args_ns.backfill
    if args_ns.log_path:
        cfg["log_path"] = args_ns.log_path
    if args_ns.poll_seconds is not None:
        cfg["poll_seconds"] = float(args_ns.poll_seconds)
    return cfg

def main():
    ap = argparse.ArgumentParser(description="Smart post-processor for files produced by an external program.")
    ap.add_argument("--config", type=str, help="Path to JSON config (optional).")
    ap.add_argument("--watch-dir", type=str, help="Directory to watch for new .txt files.")
    ap.add_argument("--extensions", type=str, nargs="+", help="File extensions to process, e.g., .txt .log")
    ap.add_argument("--pattern", type=str, help="Rename pattern (see script header for tokens).")
    ap.add_argument("--launch-exe", type=str, help="Path to BLE_CENTRAL.exe (optional launch).")
    ap.add_argument("--backfill", type=lambda s: s.lower() in ("1","true","yes","y"), help="Process existing files immediately.")
    ap.add_argument("--log-path", type=str, help="Log file path.")
    ap.add_argument("--no-watchdog", action="store_true", help="Force polling mode even if watchdog is present.")
    ap.add_argument("--poll-seconds", type=float, default=None, help="Polling interval in seconds when not using watchdog.")
    args = ap.parse_args()

    cfg = load_config(args)

    log_path = Path(cfg.get("log_path", "smart_postprocess.log"))
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_path, encoding="utf-8"),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logging.info("Config: %s", json.dumps(cfg, ensure_ascii=False))

    Path(cfg["watch_dir"]).mkdir(parents=True, exist_ok=True)

    if cfg.get("backfill_existing", True):
        backfill_existing(cfg)

    maybe_launch_exe(cfg)

    if (USE_WATCHDOG and not args.no_watchdog):
        logging.info("Using watchdog for real-time monitoring.")
        run_watchdog(cfg)
    else:
        if not USE_WATCHDOG and not args.no_watchdog:
            logging.info("watchdog not found; falling back to polling. (pip install watchdog)")
        else:
            logging.info("Using polling mode as requested.")
        run_polling(cfg)

if __name__ == "__main__":
    main()