#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Python 3.13 GPUè®­ç»ƒç¯å¢ƒè®¾ç½®
ä½¿ç”¨å¤šç§æ–¹æ³•å°è¯•å®‰è£…PyTorch GPUç‰ˆæœ¬
"""

import os
import sys
import subprocess
import urllib.request
import platform
import json

def create_gpu_config():
    """åˆ›å»ºGPUè®­ç»ƒé…ç½®"""
    config = {
        "data_dir": "./hyx_data",
        "batch_size": 16,
        "learning_rate": 1e-4,
        "num_epochs": 100,
        "patience": 15,
        "k_folds": 5,
        "test_size": 0.2,
        "model_types": ["cnn", "transformer", "fusion"],
        "use_class_weights": True,
        "augmentation": True,
        "feature_types": ["stft", "time_series"],
        "use_gpu": True,
        "device_id": 0,
        "use_amp": True,
        "compile_model": False,  # Python 3.13å¯èƒ½ä¸æ”¯æŒ
        "multi_gpu": False,
        "device_count": 1,
        "dataloader_params": {
            "num_workers": 4,
            "pin_memory": True,
            "persistent_workers": True,
            "prefetch_factor": 2,
            "drop_last": True
        }
    }
    
    with open("python313_gpu_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print("âœ… Python 3.13 GPUé…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ")

def install_pytorch_methods():
    """å°è¯•å¤šç§æ–¹æ³•å®‰è£…PyTorch GPUç‰ˆæœ¬"""
    methods = [
        {
            "name": "æ–¹æ³•1: æ ‡å‡†CUDA 12.1å®‰è£…",
            "cmd": [sys.executable, "-m", "pip", "install", "torch", "torchvision", "torchaudio", "--index-url", "https://download.pytorch.org/whl/cu121"]
        },
        {
            "name": "æ–¹æ³•2: CUDA 11.8å®‰è£…",
            "cmd": [sys.executable, "-m", "pip", "install", "torch", "torchvision", "torchaudio", "--index-url", "https://download.pytorch.org/whl/cu118"]
        },
        {
            "name": "æ–¹æ³•3: CPUç‰ˆæœ¬+æ‰‹åŠ¨CUDA",
            "cmd": [sys.executable, "-m", "pip", "install", "torch", "torchvision", "torchaudio"]
        },
        {
            "name": "æ–¹æ³•4: é¢„å‘å¸ƒç‰ˆæœ¬",
            "cmd": [sys.executable, "-m", "pip", "install", "--pre", "torch", "torchvision", "torchaudio", "--index-url", "https://download.pytorch.org/whl/nightly/cu121"]
        }
    ]
    
    for method in methods:
        print(f"\nğŸ”„ å°è¯•{method['name']}...")
        try:
            result = subprocess.run(method['cmd'], capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                print(f"âœ… {method['name']}æˆåŠŸ")
                return True
            else:
                print(f"âŒ {method['name']}å¤±è´¥: {result.stderr}")
        except subprocess.TimeoutExpired:
            print(f"â° {method['name']}è¶…æ—¶")
        except Exception as e:
            print(f"âŒ {method['name']}å¼‚å¸¸: {e}")
    
    return False

def test_gpu_functionality():
    """æµ‹è¯•GPUåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•GPUåŠŸèƒ½...")
    
    test_code = """
import torch
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')

if torch.cuda.is_available():
    print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')
    print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
    print(f'GPUåç§°: {torch.cuda.get_device_name(0)}')
    
    # æµ‹è¯•GPUè®¡ç®—
    try:
        x = torch.randn(100, 100).cuda()
        y = torch.randn(100, 100).cuda()
        z = torch.matmul(x, y)
        print('âœ… GPUè®¡ç®—æµ‹è¯•æˆåŠŸ')
        return True
    except Exception as e:
        print(f'âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}')
        return False
else:
    print('âŒ CUDAä¸å¯ç”¨')
    return False
"""
    
    try:
        exec(test_code)
        return True
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def create_fallback_script():
    """åˆ›å»ºå›é€€è„šæœ¬"""
    script_content = '''#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Python 3.13 GPUè®­ç»ƒå›é€€è„šæœ¬
å¦‚æœPyTorch GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼ä½†ä¿æŒGPUä¼˜åŒ–é…ç½®
"""

import os
import sys
import torch

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®Python 3.13è®­ç»ƒç¯å¢ƒ...")
    
    # æ£€æŸ¥PyTorch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print("âœ… GPUè®­ç»ƒç¯å¢ƒå°±ç»ª")
        return True
    else:
        print("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
        # è®¾ç½®CPUä¼˜åŒ–
        torch.set_num_threads(min(8, os.cpu_count() or 4))
        return False

def run_training():
    """è¿è¡Œè®­ç»ƒ"""
    use_gpu = setup_environment()
    
    # å¯¼å…¥è®­ç»ƒè„šæœ¬
    try:
        from run_behavior_classification import main as training_main
        
        # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°
        if use_gpu:
            sys.argv = [
                'run_behavior_classification.py',
                '--full-pipeline',
                '--config', 'python313_gpu_config.json',
                '--mixed-precision',
                '--batch-size', '16'
            ]
        else:
            sys.argv = [
                'run_behavior_classification.py',
                '--full-pipeline',
                '--config', 'python313_gpu_config.json',
                '--no-gpu',
                '--batch-size', '8'
            ]
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        training_main()
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    run_training()
'''
    
    with open("run_python313_training.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ… Python 3.13è®­ç»ƒå›é€€è„šæœ¬åˆ›å»ºå®Œæˆ")

def main():
    print("="*60)
    print("Python 3.13 GPUè®­ç»ƒç¯å¢ƒè®¾ç½®")
    print("="*60)
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}"
    print(f"Pythonç‰ˆæœ¬: {python_version}")
    
    if python_version != "3.13":
        print("âš ï¸ æ­¤è„šæœ¬ä¸“ä¸ºPython 3.13è®¾è®¡")
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    create_gpu_config()
    
    # å°è¯•å®‰è£…PyTorch
    print("\nğŸ“¦ å°è¯•å®‰è£…PyTorch GPUç‰ˆæœ¬...")
    if install_pytorch_methods():
        print("âœ… PyTorchå®‰è£…æˆåŠŸ")
        
        # æµ‹è¯•GPUåŠŸèƒ½
        if test_gpu_functionality():
            print("\nğŸ‰ Python 3.13 GPUè®­ç»ƒç¯å¢ƒé…ç½®æˆåŠŸï¼")
            print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
            print("python run_behavior_classification.py --full-pipeline --config python313_gpu_config.json --mixed-precision")
        else:
            print("\nâš ï¸ PyTorchå®‰è£…æˆåŠŸä½†GPUä¸å¯ç”¨")
            print("å¯èƒ½çš„åŸå› :")
            print("1. CUDAé©±åŠ¨æœªå®‰è£…")
            print("2. GPUä¸æ”¯æŒCUDA")
            print("3. PyTorchç‰ˆæœ¬ä¸CUDAç‰ˆæœ¬ä¸åŒ¹é…")
    else:
        print("\nâŒ æ‰€æœ‰PyTorchå®‰è£…æ–¹æ³•éƒ½å¤±è´¥äº†")
        print("å¯èƒ½çš„åŸå› :")
        print("1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("2. Python 3.13ä¸PyTorchä¸å…¼å®¹")
        print("3. é˜²ç«å¢™é˜»æ­¢ä¸‹è½½")
    
    # åˆ›å»ºå›é€€è„šæœ¬
    create_fallback_script()
    
    print("\nğŸ’¡ æ›¿ä»£æ–¹æ¡ˆ:")
    print("1. ä½¿ç”¨å›é€€è„šæœ¬: python run_python313_training.py")
    print("2. ä½¿ç”¨Dockerç¯å¢ƒ")
    print("3. é™çº§åˆ°Python 3.11/3.12")
    print("4. ç­‰å¾…å®˜æ–¹æ”¯æŒPython 3.13")

if __name__ == "__main__":
    main()
