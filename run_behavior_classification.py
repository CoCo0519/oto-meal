#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸€é”®è¿è¡Œè„šæœ¬ï¼šåŸºäºè€³é“PPG&IMUçš„è¡Œä¸ºåˆ†ç±»ç³»ç»Ÿ
æä¾›ç®€åŒ–çš„å‘½ä»¤è¡Œæ¥å£ï¼Œæ–¹ä¾¿å¿«é€Ÿå¼€å§‹è®­ç»ƒå’Œè¯„ä¼°

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python run_behavior_classification.py --quick-start
    python run_behavior_classification.py --full-pipeline --models fusion
    python run_behavior_classification.py --data-only
"""

import os
import sys
import argparse
import json
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…"""
    required_packages = [
        'torch', 'numpy', 'scipy', 'matplotlib', 'seaborn', 
        'sklearn', 'pandas', 'tqdm'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…:")
        # å°†sklearnæ˜¾ç¤ºåç§°è½¬æ¢å›scikit-learn
        display_packages = []
        for pkg in missing_packages:
            if pkg == 'sklearn':
                display_packages.append('scikit-learn')
            else:
                display_packages.append(pkg)
        
        for pkg in display_packages:
            print(f"   - {pkg}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(display_packages)}")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    return True

def check_data_directory(data_dir):
    """æ£€æŸ¥æ•°æ®ç›®å½•"""
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return False
    
    # æ£€æŸ¥å¿…è¦çš„æ•°æ®æ–‡ä»¶
    required_patterns = ['è€³é“', 'å–‰å’™']
    txt_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]
    
    if not txt_files:
        print(f"âŒ æ•°æ®ç›®å½•ä¸­æœªæ‰¾åˆ°.txtæ–‡ä»¶: {data_dir}")
        return False
    
    ear_files = [f for f in txt_files if 'è€³é“' in f]
    throat_files = [f for f in txt_files if 'å–‰å’™' in f]
    
    if not ear_files:
        print("âŒ æœªæ‰¾åˆ°è€³é“æ•°æ®æ–‡ä»¶ï¼ˆæ–‡ä»¶ååº”åŒ…å«'è€³é“'ï¼‰")
        return False
    
    if not throat_files:
        print("âŒ æœªæ‰¾åˆ°å–‰å’™æ•°æ®æ–‡ä»¶ï¼ˆæ–‡ä»¶ååº”åŒ…å«'å–‰å’™'ï¼‰")
        return False
    
    print(f"âœ… æ•°æ®ç›®å½•æ£€æŸ¥é€šè¿‡: {data_dir}")
    print(f"   - è€³é“æ–‡ä»¶: {len(ear_files)}ä¸ª")
    print(f"   - å–‰å’™æ–‡ä»¶: {len(throat_files)}ä¸ª")
    
    return True

def run_data_labeling_only(data_dir):
    """ä»…è¿è¡Œæ•°æ®æ ‡æ³¨"""
    print("\n" + "="*50)
    print("è¿è¡Œæ•°æ®æ ‡æ³¨æ¨¡å—")
    print("="*50)
    
    try:
        from data_labeling_system import main as labeling_main
        
        # ä¸´æ—¶ä¿®æ”¹sys.argvä»¥ä¼ é€’å‚æ•°
        original_argv = sys.argv.copy()
        sys.argv = ['data_labeling_system.py']
        
        labeling_main()
        
        # æ¢å¤åŸå§‹argv
        sys.argv = original_argv
        
        print("âœ… æ•°æ®æ ‡æ³¨å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ ‡æ³¨å¤±è´¥: {e}")
        return False
    
    return True

def run_feature_extraction_only(data_dir):
    """ä»…è¿è¡Œç‰¹å¾æå–å’ŒåŸºç¡€åˆ†ç±»"""
    print("\n" + "="*50)
    print("è¿è¡Œç‰¹å¾æå–å’ŒåŸºç¡€åˆ†ç±»")
    print("="*50)
    
    try:
        from behavior_classification_system import main as classification_main
        
        # ä¸´æ—¶ä¿®æ”¹sys.argv
        original_argv = sys.argv.copy()
        sys.argv = ['behavior_classification_system.py']
        
        classification_main()
        
        # æ¢å¤åŸå§‹argv
        sys.argv = original_argv
        
        print("âœ… ç‰¹å¾æå–å’ŒåŸºç¡€åˆ†ç±»å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
        return False
    
    return True

def run_full_pipeline(data_dir, models, config):
    """è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿"""
    print("\n" + "="*50)
    print("è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿")
    print("="*50)
    
    try:
        from complete_training_pipeline import TrainingPipeline
        
        # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
        temp_config = {
            'data_dir': data_dir,
            'model_types': models,
            **config
        }
        
        config_path = 'temp_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(temp_config, f, ensure_ascii=False, indent=2)
        
        # åˆå§‹åŒ–å¹¶è¿è¡Œæµæ°´çº¿
        pipeline = TrainingPipeline(config_path)
        results = pipeline.run_complete_pipeline()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(config_path):
            os.remove(config_path)
        
        print("âœ… å®Œæ•´è®­ç»ƒæµæ°´çº¿æ‰§è¡Œå®Œæˆ")
        print(f"   ç»“æœä¿å­˜åœ¨: {pipeline.output_dir}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæµæ°´çº¿å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_quick_start_config():
    """åˆ›å»ºå¿«é€Ÿå¼€å§‹é…ç½®"""
    return {
        'batch_size': 16,  # è¾ƒå°çš„æ‰¹æ¬¡ä»¥é€‚åº”æ›´å¤šè®¾å¤‡
        'learning_rate': 1e-4,
        'num_epochs': 50,  # è¾ƒå°‘çš„è½®æ•°ç”¨äºå¿«é€ŸéªŒè¯
        'patience': 10,
        'test_size': 0.2,
        'use_class_weights': True,
        'augmentation': False  # å…³é—­æ•°æ®å¢å¼ºä»¥åŠ å¿«é€Ÿåº¦
    }

def main():
    parser = argparse.ArgumentParser(
        description='åŸºäºè€³é“PPG&IMUçš„è¡Œä¸ºåˆ†ç±»ç³»ç»Ÿ - ä¸€é”®è¿è¡Œè„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --quick-start                    # å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èæ–°æ‰‹ï¼‰
  %(prog)s --full-pipeline                  # è¿è¡Œå®Œæ•´æµæ°´çº¿
  %(prog)s --data-only                      # ä»…æ•°æ®æ ‡æ³¨
  %(prog)s --feature-only                   # ä»…ç‰¹å¾æå–
  %(prog)s --models cnn transformer         # æŒ‡å®šæ¨¡å‹ç±»å‹
  %(prog)s --config config_example.json     # ä½¿ç”¨é…ç½®æ–‡ä»¶
        """
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--data-dir', type=str, default='./hyx_data',
                       help='æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: ./hyx_data)')
    
    # è¿è¡Œæ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('--quick-start', action='store_true',
                           help='å¿«é€Ÿå¼€å§‹æ¨¡å¼ï¼ˆæ¨èæ–°æ‰‹ä½¿ç”¨ï¼‰')
    mode_group.add_argument('--full-pipeline', action='store_true',
                           help='è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿')
    mode_group.add_argument('--data-only', action='store_true',
                           help='ä»…è¿è¡Œæ•°æ®æ ‡æ³¨')
    mode_group.add_argument('--feature-only', action='store_true',
                           help='ä»…è¿è¡Œç‰¹å¾æå–å’ŒåŸºç¡€åˆ†ç±»')
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument('--models', nargs='+', 
                       choices=['cnn', 'transformer', 'fusion'],
                       default=['fusion'],
                       help='è¦è®­ç»ƒçš„æ¨¡å‹ç±»å‹ (é»˜è®¤: fusion)')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=None,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=None,
                       help='å­¦ä¹ ç‡')
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--no-gpu', action='store_true',
                       help='ç¦ç”¨GPUåŠ é€Ÿ')
    parser.add_argument('--verbose', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("ğŸ¯ åŸºäºè€³é“PPG&IMUçš„è¡Œä¸ºåˆ†ç±»ç³»ç»Ÿ")
    print("   ç›®æ ‡ï¼šé™æ¯/å’€åš¼/å’³å—½/åå’½ å››åˆ†ç±»")
    print("   æŠ€æœ¯ï¼šæ·±åº¦å­¦ä¹  + å¤šæ¨¡æ€ç‰¹å¾èåˆ")
    print()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return 1
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not check_data_directory(args.data_dir):
        print("\nğŸ’¡ æç¤º:")
        print("   1. ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨ä¸”åŒ…å«.txtæ–‡ä»¶")
        print("   2. æ–‡ä»¶ååº”åŒ…å«'è€³é“'å’Œ'å–‰å’™'å…³é”®è¯")
        print("   3. æ•°æ®æ ¼å¼ï¼šé¦–è¡Œä¸ºä¸­æ–‡è¡¨å¤´ï¼Œ6åˆ—æ•°å€¼æ•°æ®")
        return 1
    
    # è®¾ç½®GPU
    if args.no_gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        print("ğŸš« å·²ç¦ç”¨GPUåŠ é€Ÿ")
    
    # åŠ è½½é…ç½®
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"ğŸ“‹ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    else:
        config = create_quick_start_config()
        if args.quick_start:
            print("ğŸš€ ä½¿ç”¨å¿«é€Ÿå¼€å§‹é…ç½®")
    
    # è¦†ç›–é…ç½®å‚æ•°
    if args.epochs is not None:
        config['num_epochs'] = args.epochs
    if args.batch_size is not None:
        config['batch_size'] = args.batch_size
    if args.lr is not None:
        config['learning_rate'] = args.lr
    
    # è¿è¡Œå¯¹åº”æ¨¡å¼
    success = False
    
    if args.data_only:
        success = run_data_labeling_only(args.data_dir)
        
    elif args.feature_only:
        success = run_feature_extraction_only(args.data_dir)
        
    elif args.full_pipeline or args.quick_start:
        if args.quick_start:
            print("ğŸ¯ å¿«é€Ÿå¼€å§‹æ¨¡å¼ï¼šå°†è®­ç»ƒèåˆæ¨¡å‹ï¼ˆæ¨èï¼‰")
            models = ['fusion']
        else:
            models = args.models
            
        success = run_full_pipeline(args.data_dir, models, config)
    
    # æ€»ç»“
    print("\n" + "="*60)
    if success:
        print("ğŸ‰ æ‰§è¡Œå®Œæˆï¼")
        if args.full_pipeline or args.quick_start:
            print("ğŸ“Š æŸ¥çœ‹ç»“æœï¼š")
            print("   - è®­ç»ƒæ—¥å¿—ï¼štraining.log")
            print("   - å¯è§†åŒ–ç»“æœï¼šcomprehensive_results.png") 
            print("   - æ¨¡å‹æ–‡ä»¶ï¼šbest_*_model.pth")
            print("   - è¯¦ç»†æŠ¥å‘Šï¼šclassification_report.txt")
    else:
        print("âŒ æ‰§è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹é”™è¯¯ä¿¡æ¯")
        return 1
    
    print("="*60)
    return 0

if __name__ == "__main__":
    sys.exit(main())
