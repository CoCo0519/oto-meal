#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
run_behavior_classification.py
ç»Ÿä¸€å…¥å£ï¼ˆä¿®å¤+ä¼˜åŒ–ç‰ˆï¼‰
- å…ˆè§£æ CLI/è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œå†åšåç«¯ä¼˜åŒ–ä¸æ—¥å¿—
- è‡ªæ£€ï¼šGPU/AMP æ­£å¸¸ï¼›æœªè£… Triton æ—¶è‡ªåŠ¨è·³è¿‡ torch.compile æµ‹è¯•
- DataLoader å‚æ•°æ›´ç¨³å¥ï¼ˆä»…åœ¨ num_workers>0 æ—¶è®¾ç½® prefetch_factorï¼‰
- å…¼å®¹è€æ—§æ¨¡å—ï¼šå¯é€‰æ³¨å…¥ torch/optim/nn/Fï¼ˆè¡Œä¸ºæ¨¡å—ç°åœ¨å·²ä¿®å¤ä¸ºæ˜¾å¼ importï¼Œä¸å†ä¾èµ–æ³¨å…¥ï¼‰
"""

from __future__ import annotations

import os
import sys
import json
import glob
import shutil
import argparse
import platform
from datetime import datetime
import traceback
from pathlib import Path
import importlib
import warnings

# --------------------------- CLI æ„å»º ---------------------------

def build_arg_parser():
    parser = argparse.ArgumentParser(
        description="è¡Œä¸ºåˆ†ç±»è®­ç»ƒ/æ¨ç†ç»Ÿä¸€å…¥å£ï¼ˆæ”¯æŒ *_data å¤šç›®å½•ï¼‰",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog=(
            "ç¤ºä¾‹ï¼š\n"
            "  %(prog)s --quick-start                   # å¿«é€Ÿè·‘ä¸€éï¼ˆç­‰ä»·äºç‰¹å¾æå–ï¼‰\n"
            "  %(prog)s --full-pipeline                 # è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿\n"
            "  %(prog)s --data-only                     # ä»…æ•°æ®æ ‡æ³¨/å‡†å¤‡\n"
            "  %(prog)s --feature-only                  # ä»…ç‰¹å¾æå–\n"
            "  %(prog)s --models cnn transformer        # æŒ‡å®šæ¨¡å‹ç±»å‹\n"
            "  %(prog)s --config config_example.json    # ä½¿ç”¨é…ç½®æ–‡ä»¶\n"
            "  %(prog)s --data-glob \"./*_data\"          # ä½¿ç”¨æ‰€æœ‰ *_data ç›®å½•\n"
            "\n"
            "GPUä¼˜åŒ–ç¤ºä¾‹ï¼š\n"
            "  %(prog)s --test-gpu                           # æµ‹è¯•GPUåŠŸèƒ½\n"
            "  %(prog)s --full-pipeline --mixed-precision    # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\n"
            "  %(prog)s --full-pipeline --batch-size 64       # è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°\n"
            "  %(prog)s --full-pipeline --gpu-id 0            # æŒ‡å®šGPUè®¾å¤‡\n"
            "  %(prog)s --full-pipeline --gpu-memory-fraction 0.8  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨\n"
            "  %(prog)s --full-pipeline --no-gpu              # å¼ºåˆ¶ä½¿ç”¨CPU\n"
        )
    )

    mode = parser.add_mutually_exclusive_group()
    mode.add_argument('--quick-start', action='store_true', help='å¿«é€Ÿè¿è¡Œï¼ˆé»˜è®¤èµ°ç‰¹å¾æå–ï¼‰')
    mode.add_argument('--full-pipeline', action='store_true', help='è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿')
    mode.add_argument('--data-only', action='store_true', help='ä»…æ•°æ®æ ‡æ³¨/å‡†å¤‡')
    mode.add_argument('--feature-only', action='store_true', help='ä»…ç‰¹å¾æå–')

    parser.add_argument('--data-dir', type=str, default='./hyx_data', help='æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: ./hyx_data)')
    parser.add_argument('--data-glob', type=str, default=None, help='æ•°æ®ç›®å½•é€šé…ç¬¦ï¼Œä¾‹å¦‚ \"./*_data\"ï¼ˆå°†è‡ªåŠ¨åˆå¹¶å…¶ä¸‹æ‰€æœ‰ .txtï¼‰')
    parser.add_argument('--merge-strategy', type=str, default='copy', choices=['copy', 'link'], help='åˆå¹¶æ•°æ®æ—¶ç­–ç•¥ï¼šcopy/ç¡¬é“¾æ¥(link)ã€‚Windows ä¸Šæ¨è copyã€‚')
    parser.add_argument('--models', nargs='+', default=['fusion'], help='æŒ‡å®šæ¨¡å‹ç±»å‹åˆ—è¡¨ï¼Œä¾‹å¦‚: --models cnn transformer fusion')
    parser.add_argument('--config', type=str, default=None, help='é…ç½® JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¯åŒ…å« data_glob/dataloader/gpu ç­‰å‚æ•°ï¼‰')

    # GPU ç›¸å…³
    parser.add_argument('--no-gpu', action='store_true', help='ç¦ç”¨GPUåŠ é€Ÿ')
    parser.add_argument('--gpu-id', type=int, default=None, help='æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ID')
    parser.add_argument('--batch-size', type=int, default=None, help='æ‰¹æ¬¡å¤§å°ï¼ˆGPUæ¨¡å¼ä¸‹ä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼‰')
    parser.add_argument('--mixed-precision', action='store_true', help='å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹GPUæ”¯æŒï¼‰')
    parser.add_argument('--no-mixed-precision', action='store_true', help='ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--gpu-memory-fraction', type=float, default=0.9, help='GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹ (0.1-1.0, é»˜è®¤0.9)')
    parser.add_argument('--test-gpu', action='store_true', help='ä»…æµ‹è¯•GPUåŠŸèƒ½ï¼Œä¸è¿è¡Œè®­ç»ƒ')

    # å¼€å…³ï¼šæ˜¯å¦æµ‹è¯• torch.compileï¼ˆTriton ç¼ºå¤±ä¼šå¤±è´¥ï¼‰
    parser.add_argument('--skip-compile-test', action='store_true', help='è·³è¿‡ torch.compile æµ‹è¯•ï¼ˆæœªè£… Triton æ—¶å»ºè®®å¼€å¯ï¼‰')

    return parser

# --------------------------- GPU/ç¯å¢ƒå·¥å…· ---------------------------

def ensure_torch_cuda_build():
    """ç¡®ä¿ä¸º CUDA æ„å»ºï¼›è‹¥æ£€æµ‹åˆ° +cpu ç‰ˆæœ¬ï¼Œç»™å‡ºä¿®å¤å»ºè®®"""
    try:
        import torch
        built_cuda = getattr(torch.version, 'cuda', None)
        print(f"  Torch: {getattr(torch, '__version__', 'unknown')}  built cuda: {built_cuda or '[cpu build]'}")
        if (not built_cuda) or ('cpu' in str(getattr(torch, '__version__', '')).lower()):
            print("  âš ï¸ æ£€æµ‹åˆ° CPU æ„å»ºçš„ PyTorchã€‚è‹¥æœŸæœ›ä½¿ç”¨ GPUï¼Œè¯·å¸è½½å¹¶å®‰è£… CUDA ç‰ˆï¼š")
            print("     pip uninstall -y torch torchvision torchaudio")
            print("     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124")
            return False
        return True
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•æ£€æµ‹ torch æ„å»ºç±»å‹ï¼š{e}")
        return False

def setup_gpu_performance(precision: str = "high", mem_fraction: float | None = None):
    """ä¼˜åŒ– PyTorch åç«¯ä»¥æå‡ GPU åˆ©ç”¨ç‡ï¼ˆè‹¥å¯ç”¨ï¼‰"""
    try:
        import torch
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            if hasattr(torch, "set_float32_matmul_precision"):
                torch.set_float32_matmul_precision(precision)
            torch.cuda.empty_cache()
            if mem_fraction is not None and hasattr(torch.cuda, 'set_per_process_memory_fraction'):
                try:
                    torch.cuda.set_per_process_memory_fraction(float(mem_fraction))
                except Exception as e:
                    print(f"  âš ï¸ è®¾ç½®æ¯è¿›ç¨‹æ˜¾å­˜æ¯”ä¾‹å¤±è´¥ï¼š{e}")
            if 'CUDA_VISIBLE_DEVICES' in os.environ:
                vis = os.environ['CUDA_VISIBLE_DEVICES']
                if vis:
                    try:
                        torch.cuda.set_device(int(vis.split(',')[0]))
                    except Exception:
                        pass
            print("âœ… GPU æ€§èƒ½ä¼˜åŒ–å·²å¯ç”¨ï¼ˆTF32/CuDNN/benchmarksï¼‰")
        else:
            try:
                torch.set_num_threads(max(1, (os.cpu_count() or 2)//2))
                print("âš ï¸ ä½¿ç”¨ CPU æ¨¡å¼ï¼Œå·²ä¼˜åŒ–çº¿ç¨‹æ•°")
            except Exception:
                pass
    except ImportError:
        print("âš ï¸ PyTorch æœªå®‰è£…ï¼Œè·³è¿‡ GPU ä¼˜åŒ–è®¾ç½®")
    except Exception as e:
        print(f"âš ï¸ GPU æ€§èƒ½ä¼˜åŒ–è®¾ç½®å¤±è´¥ï¼š{e}")

def get_gpu_info():
    """è·å–è¯¦ç»†çš„GPUä¿¡æ¯"""
    try:
        import torch
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            info = {'available': True, 'device_count': device_count, 'current_device': current_device, 'devices': []}
            for i in range(device_count):
                props = torch.cuda.get_device_properties(i)
                cc = float(f"{props.major}.{props.minor}")
                info['devices'].append({
                    'id': i,
                    'name': props.name,
                    'memory_total': props.total_memory / (1024**3),
                    'memory_allocated': torch.cuda.memory_allocated(i) / (1024**3),
                    'memory_reserved': torch.cuda.memory_reserved(i) / (1024**3),
                    'compute_capability': cc,
                    'multiprocessor_count': props.multi_processor_count
                })
            return info
        else:
            return {'available': False}
    except Exception as e:
        print(f"âš ï¸ è·å–GPUä¿¡æ¯å¤±è´¥ï¼š{e}")
        return {'available': False}

def optimize_data_loading_for_gpu():
    """ä¸ºGPUè®­ç»ƒä¼˜åŒ–æ•°æ®åŠ è½½å‚æ•°"""
    cpu_count = os.cpu_count() or 2
    try:
        import torch
        if torch.cuda.is_available():
            return {
                'num_workers': min(cpu_count, 8),
                'pin_memory': True,
                'persistent_workers': True,
                'prefetch_factor': 4,  # ä»…å½“ num_workers>0 æ—¶æœ‰æ•ˆ
                'drop_last': True
            }
        else:
            return {
                'num_workers': max(1, cpu_count // 2),
                'pin_memory': False,
                'persistent_workers': False,
                'prefetch_factor': 2,
                'drop_last': False
            }
    except Exception:
        return {
            'num_workers': 2,
            'pin_memory': False,
            'persistent_workers': False,
            'prefetch_factor': 2,
            'drop_last': False
        }

def log_gpu_environment():
    """æ‰“å°ä¸ GPU/CPU ç¯å¢ƒç›¸å…³çš„ä¿¡æ¯"""
    print("\n[ç¯å¢ƒä¿¡æ¯]")
    try:
        import torch
        print(f"  Python: {platform.python_version()}  Torch: {getattr(torch, '__version__', 'unknown')}")
        ensure_torch_cuda_build()

        gpu_info = get_gpu_info()
        if gpu_info['available']:
            d = gpu_info['devices'][gpu_info['current_device']]
            cudnn_ver = getattr(torch.backends, 'cudnn', None)
            cudnn_ver = getattr(cudnn_ver, 'version', lambda: None)()
            print(f"  CUDA å¯ç”¨ âœ”  è®¾å¤‡: {d['name']}")
            print(f"  Compute Capability: {d['compute_capability']}")
            print(f"  æ˜¾å­˜æ€»é‡: {d['memory_total']:.1f} GiB  å·²åˆ†é…: {d['memory_allocated']:.2f} GiB  å·²ä¿ç•™: {d['memory_reserved']:.2f} GiB")
            print(f"  cuDNN ç‰ˆæœ¬: {cudnn_ver}")
            print(f"  cudnn.benchmark: {getattr(torch.backends.cudnn, 'benchmark', None)}")
            print(f"  allow_tf32: {getattr(torch.backends.cuda.matmul, 'allow_tf32', None)}")
            if hasattr(torch.cuda, 'amp'):
                print("  âœ… æ”¯æŒ AMP (è‡ªåŠ¨æ··åˆç²¾åº¦)")
        else:
            print("  CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
            print(f"  CPU æ ¸å¿ƒæ•°: {os.cpu_count()}")
    except ImportError:
        print(f"  Python: {platform.python_version()}  Torch: æœªå®‰è£…")
        print("  âš ï¸ PyTorch æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼")
        print(f"  CPU æ ¸å¿ƒæ•°: {os.cpu_count()}")
    except Exception as e:
        print(f"âš ï¸ ç¯å¢ƒä¿¡æ¯æ‰“å°å¤±è´¥ï¼š{e}")

def create_gpu_optimized_config(user_args=None):
    """åˆ›å»ºGPUä¼˜åŒ–çš„è®­ç»ƒé…ç½®ï¼ˆç»“åˆè®¾å¤‡ä¿¡æ¯ä¸ CLI è¦†ç›–ï¼‰"""
    gpu_info = get_gpu_info()
    base = {
        'use_gpu': gpu_info['available'],
        'mixed_precision': False,
        'gradient_accumulation_steps': 1,
        'gradient_clip_norm': 1.0,
        'dataloader_params': optimize_data_loading_for_gpu(),
        'device_id': 0,
        'use_amp': False,
        'compile_model': False
    }

    if gpu_info['available']:
        d = gpu_info['devices'][gpu_info['current_device']]
        vram = d['memory_total']
        cc = d['compute_capability']

        if vram >= 16:
            base['batch_size'] = 64; base['mixed_precision'] = True; base['use_amp'] = True
        elif vram >= 8:
            base['batch_size'] = 32; base['mixed_precision'] = True; base['use_amp'] = True
        elif vram >= 4:
            base['batch_size'] = 16; base['mixed_precision'] = True; base['use_amp'] = True
        else:
            base['batch_size'] = 8;  base['mixed_precision'] = False; base['use_amp'] = False

        base['use_tensor_cores'] = cc >= 7.0
        base['compile_model'] = cc >= 7.0

        if gpu_info['device_count'] > 1:
            base['multi_gpu'] = True
            base['device_count'] = gpu_info['device_count']
            base['batch_size'] *= gpu_info['device_count']
        else:
            base['multi_gpu'] = False
            base['device_count'] = 1
    else:
        base.update({
            'batch_size': 8,
            'mixed_precision': False,
            'use_tensor_cores': False,
            'multi_gpu': False,
            'device_count': 1,
            'use_amp': False,
            'compile_model': False
        })

    if user_args is not None:
        if user_args.batch_size is not None:
            base['batch_size'] = user_args.batch_size
        if user_args.mixed_precision:
            base['mixed_precision'] = True; base['use_amp'] = True
        if user_args.no_mixed_precision:
            base['mixed_precision'] = False; base['use_amp'] = False
        if user_args.gpu_id is not None:
            base['device_id'] = user_args.gpu_id

    dl = base['dataloader_params']
    if not dl or dl.get('num_workers', 0) <= 0:
        dl.pop('prefetch_factor', None)

    return base

def monitor_gpu_memory():
    """ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        import torch
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024**3)
            reserved = torch.cuda.memory_reserved() / (1024**3)
            total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"  GPUå†…å­˜: {allocated:.2f}GB / {reserved:.2f}GB / {total:.2f}GB (å·²ç”¨/ä¿ç•™/æ€»é‡)")
            if allocated / total > 0.9:
                print("  âš ï¸ GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®æ¸…ç†ç¼“å­˜")
                import torch as _t; _t.cuda.empty_cache()
                return True
        return False
    except Exception:
        return False

def _has_triton():
    try:
        import triton  # noqa: F401
        return True
    except Exception:
        return False

def test_gpu_functionality(skip_compile_test: bool = False):
    """æµ‹è¯•GPUåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("\nğŸ§ª æµ‹è¯•GPUåŠŸèƒ½...")
    try:
        import torch
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUæµ‹è¯•")
            return False

        device = torch.device('cuda')
        print(f"âœ… ä½¿ç”¨GPUè®¾å¤‡: {torch.cuda.get_device_name()}")

        x = torch.randn(1024, 1024, device=device)
        y = torch.randn(1024, 1024, device=device)
        _ = torch.matmul(x, y)
        print("âœ… GPUå¼ é‡è¿ç®—æ­£å¸¸")

        try:
            if hasattr(torch.amp, 'autocast'):
                with torch.amp.autocast('cuda'):
                    x = torch.randn(512, 512, device=device)
                    y = torch.randn(512, 512, device=device)
                    _ = torch.matmul(x, y)
                print("âœ… æ··åˆç²¾åº¦ (AMP) æ­£å¸¸")
        except Exception as e:
            print(f"âš ï¸ æ··åˆç²¾åº¦æµ‹è¯•å¤±è´¥: {e}")

        try:
            if not skip_compile_test and hasattr(torch, 'compile'):
                if _has_triton():
                    mdl = torch.nn.Linear(256, 128).to(device)
                    cmpl = torch.compile(mdl)
                    _ = cmpl(torch.randn(4, 256, device=device))
                    print("âœ… æ¨¡å‹ç¼–è¯‘ (torch.compile) æ­£å¸¸")
                else:
                    print("â„¹ï¸ æœªæ£€æµ‹åˆ° Tritonï¼Œå·²è·³è¿‡ torch.compile æµ‹è¯•ï¼ˆå¦‚éœ€å¯ç”¨è¯·å®‰è£… tritonï¼‰")
            elif skip_compile_test:
                print("â„¹ï¸ æŒ‰å‚æ•°è¦æ±‚è·³è¿‡ torch.compile æµ‹è¯•")
            else:
                print("â„¹ï¸ å½“å‰ PyTorch ä¸æ”¯æŒ torch.compile")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ç¼–è¯‘æµ‹è¯•å¤±è´¥: {e}")

        import torch as _t; _t.cuda.empty_cache()
        print("âœ… GPUåŠŸèƒ½æµ‹è¯•å®Œæˆ")
        return True
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡ŒGPUæµ‹è¯•")
        return False
    except Exception as e:
        print(f"âŒ GPUåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def setup_plot_fonts():
    """ä¸º matplotlib é…ç½®ä¸­æ–‡å­—ä½“ä¸è´Ÿå·æ˜¾ç¤ºï¼ˆè‹¥ matplotlib å¯ç”¨ï¼‰ã€‚"""
    try:
        import matplotlib
        import matplotlib.pyplot as plt  # noqa: F401
        for name in ["SimHei", "Microsoft YaHei", "Arial Unicode MS"]:
            try:
                matplotlib.rcParams['font.sans-serif'] = [name]
                break
            except Exception:
                continue
        matplotlib.rcParams['axes.unicode_minus'] = False
    except Exception:
        pass

# --------------------------- æ•°æ®å·¥å…· ---------------------------

def check_data_directory(data_dir: str) -> bool:
    p = Path(data_dir)
    if not p.exists() or not p.is_dir():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼š{p.resolve()}")
        return False
    txts = list(p.rglob("*.txt"))
    if not txts:
        print(f"âŒ æ•°æ®ç›®å½•ä¸­æœªæ‰¾åˆ° .txt æ•°æ®æ–‡ä»¶ï¼š{p.resolve()}")
        return False
    print(f"âœ… æ•°æ®ç›®å½•æ£€æŸ¥é€šè¿‡ï¼š{p.resolve()}ï¼ˆ{len(txts)} ä¸ª .txtï¼‰")
    return True

def gather_data_to_merged_dir(data_glob: str, merge_strategy: str = "copy") -> str:
    merged = Path("./_merged_data")
    if merged.exists():
        shutil.rmtree(merged)
    merged.mkdir(parents=True, exist_ok=True)

    folders = [Path(p) for p in glob.glob(data_glob) if Path(p).is_dir()]
    if not folders:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®æ–‡ä»¶å¤¹ï¼š{data_glob}")
        return str(merged)

    count = 0
    for folder in folders:
        for txt in folder.rglob("*.txt"):
            dst = merged / f"{folder.name}__{txt.name}"
            if merge_strategy == "link" and hasattr(os, 'link'):
                try:
                    os.link(txt, dst)
                except Exception:
                    shutil.copy2(txt, dst)
            else:
                shutil.copy2(txt, dst)
            count += 1

    print(f"âœ… å·²æ”¶é›† {count} ä¸ª .txt æ–‡ä»¶åˆ° {merged.resolve()}")
    return str(merged)

# --------------------------- ï¼ˆå¯é€‰ï¼‰è¡Œä¸ºæ¨¡å—æ³¨å…¥ ---------------------------

def patch_behavior_module_symbols(mod):
    """è€ä»£ç å…œåº•ï¼šæŠŠ optim/nn/F/torch æ³¨å…¥åˆ°æ¨¡å—å‘½åç©ºé—´ï¼ˆæ–°ç‰ˆæœ¬å·²æ˜¾å¼ importï¼Œä¸å†ä¾èµ–ï¼‰"""
    try:
        import torch
        import torch.optim as optim
        import torch.nn as nn
        import torch.nn.functional as F
        for name, obj in [('optim', optim), ('nn', nn), ('F', F), ('torch', torch)]:
            if not hasattr(mod, name):
                setattr(mod, name, obj)
    except Exception as e:
        print(f"âš ï¸ æ³¨å…¥ torch ç¬¦å·å¤±è´¥ï¼š{e}")

# --------------------------- å­æµç¨‹ ---------------------------

def run_data_labeling_only(data_dir: str) -> bool:
    print("\n" + "="*50)
    print("ä»…è¿è¡Œæ•°æ®æ ‡æ³¨ / åŸºç¡€å¤„ç†")
    print("="*50)

    if not check_data_directory(data_dir):
        return False

    try:
        mod = importlib.import_module('behavior_classification_system')
        patch_behavior_module_symbols(mod)
        classification_main = getattr(mod, 'main')

        original_argv = sys.argv.copy()
        try:
            sys.argv = ['behavior_classification_system.py', '--data-dir', data_dir, '--feature-only']
            classification_main()
        except SystemExit:
            try:
                sys.argv = ['behavior_classification_system.py', '--data_dir', data_dir, '--feature-only']
                classification_main()
            except SystemExit:
                sys.argv = ['behavior_classification_system.py', '--data-dir', data_dir]
                classification_main()
        finally:
            sys.argv = original_argv
        print("âœ… æ•°æ®æ ‡æ³¨/åŸºç¡€å¤„ç†å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®æ ‡æ³¨/åŸºç¡€å¤„ç†å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def run_feature_extraction_only(data_dir: str) -> bool:
    print("\n" + "="*50)
    print("ä»…è¿è¡Œç‰¹å¾æå–")
    print("="*50)

    if not check_data_directory(data_dir):
        return False

    try:
        mod = importlib.import_module('behavior_classification_system')
        patch_behavior_module_symbols(mod)
        classification_main = getattr(mod, 'main')

        original_argv = sys.argv.copy()
        try:
            sys.argv = ['behavior_classification_system.py', '--data-dir', data_dir, '--data-only']
            classification_main()
        except SystemExit:
            try:
                sys.argv = ['behavior_classification_system.py', '--data_dir', data_dir, '--data-only']
                classification_main()
            except SystemExit:
                sys.argv = ['behavior_classification_system.py', '--data-dir', data_dir]
                classification_main()
        finally:
            sys.argv = original_argv
        print("âœ… ç‰¹å¾æå–å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def run_simple_cpu_training(data_dir: str, models: list[str], config: dict, args=None) -> bool:
    print("\n" + "="*50)
    print("è¿è¡Œç®€åŒ–CPUè®­ç»ƒæ¨¡å¼")
    print("="*50)

    if not check_data_directory(data_dir):
        return False

    try:
        print("ğŸ“Š å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        data_files = list(Path(data_dir).rglob("*.txt"))
        print(f"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")

        total_samples = 0
        for file_path in data_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    total_samples += len(lines)
                    print(f"  {file_path.name}: {len(lines)} è¡Œæ•°æ®")
            except Exception as e:
                print(f"  âš ï¸ è¯»å– {file_path.name} å¤±è´¥: {e}")

        print(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"  - æ€»æ–‡ä»¶æ•°: {len(data_files)}")
        print(f"  - æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  - å¹³å‡æ¯æ–‡ä»¶: {total_samples/len(data_files):.1f} æ ·æœ¬")

        report_path = f"simple_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ç®€åŒ–CPUè®­ç»ƒåˆ†ææŠ¥å‘Š\n")
            f.write("="*50 + "\n")
            f.write(f"æ•°æ®ç›®å½•: {data_dir}\n")
            f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ–‡ä»¶æ•°: {len(data_files)}\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {total_samples}\n")
            f.write(f"å¹³å‡æ¯æ–‡ä»¶: {total_samples/len(data_files):.1f} æ ·æœ¬\n\n")
            f.write("æ–‡ä»¶åˆ—è¡¨:\n")
            for file_path in data_files:
                f.write(f"  - {file_path.name}\n")

        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print("âœ… ç®€åŒ–CPUè®­ç»ƒå®Œæˆ")
        print("\nğŸ’¡ æç¤º: è¦ä½¿ç”¨å®Œæ•´çš„GPUè®­ç»ƒåŠŸèƒ½ï¼Œè¯·å®‰è£… PyTorch CUDA æ„å»ºï¼š")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124")
        return True

    except Exception as e:
        print(f"âŒ ç®€åŒ–CPUè®­ç»ƒå¤±è´¥: {e}")
        traceback.print_exc()
        return False

def run_full_pipeline(data_dir: str, models: list[str], config: dict, args=None) -> bool:
    print("\n" + "="*50)
    print("è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿")
    print("="*50)

    if not check_data_directory(data_dir):
        return False

    try:
        import torch
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            if hasattr(torch, "set_float32_matmul_precision"):
                torch.set_float32_matmul_precision("high")
    except ImportError:
        print("âš ï¸ PyTorch æœªå®‰è£…ï¼Œè·³è¿‡GPUä¼˜åŒ–")
    except Exception:
        pass

    try:
        from complete_training_pipeline import TrainingPipeline  # æŒ‰é¡¹ç›®ç»“æ„è°ƒæ•´
    except ImportError:
        print("âš ï¸ complete_training_pipeline æ¨¡å—æœªæ‰¾åˆ°")
        print("å°è¯•ä½¿ç”¨ç®€åŒ–çš„CPUæ¨¡å¼...")
        return run_simple_cpu_training(data_dir, models, config, args)

    gpu_cfg = create_gpu_optimized_config(args)

    tmp = {
        'data_dir': data_dir,
        'model_types': models,
        **gpu_cfg,
        **{k: v for k, v in config.items() if k not in ('data_dir', 'model_types')}
    }
    if 'dataloader_params' in config:
        tmp['dataloader_params'].update(config['dataloader_params'])

    print(f"ğŸš€ GPUä¼˜åŒ–é…ç½®:")
    print(f"   - ä½¿ç”¨GPU: {tmp['use_gpu']}")
    print(f"   - æ··åˆç²¾åº¦: {tmp['mixed_precision']}  (AMP: {tmp.get('use_amp', False)})")
    print(f"   - æ‰¹æ¬¡å¤§å°: {tmp['batch_size']}")
    print(f"   - è®¾å¤‡ID: {tmp.get('device_id', 0)}")
    print(f"   - æ•°æ®åŠ è½½å™¨workers: {tmp['dataloader_params']['num_workers']}")
    pf = tmp['dataloader_params'].get('prefetch_factor', None)
    print(f"   - é¢„å–å› å­: {pf if pf is not None else '(æœªè®¾ç½®æˆ–æ— æ•ˆ)'}")
    print(f"   - æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–: {tmp.get('compile_model', False)}")
    if tmp.get('multi_gpu'):
        print(f"   - å¤šGPUè®­ç»ƒ: {tmp['device_count']} ä¸ªGPU")

    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        config_path = f"./_auto_config_{ts}.json"
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(tmp, f, ensure_ascii=False, indent=2)

        pipeline = TrainingPipeline(config_path)

        if tmp['use_gpu']:
            print("\nğŸ“Š è®­ç»ƒå‰GPUçŠ¶æ€:")
            monitor_gpu_memory()

        results = pipeline.run_complete_pipeline()

        if tmp['use_gpu']:
            print("\nğŸ§¹ è®­ç»ƒåGPUæ¸…ç†:")
            monitor_gpu_memory()

        if os.path.exists(config_path):
            os.remove(config_path)

        print("âœ… å®Œæ•´è®­ç»ƒæµæ°´çº¿æ‰§è¡Œå®Œæˆ")
        print(f"   ç»“æœä¿å­˜åœ¨: {getattr(pipeline, 'output_dir', '[pipeline.output_dir ä¸å¯ç”¨]')}")
        return True

    except Exception as e:
        print(f"âŒ è®­ç»ƒæµæ°´çº¿å¤±è´¥: {e}")
        traceback.print_exc()
        print("============================================================")
        print("âŒ æ‰§è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹é”™è¯¯ä¿¡æ¯")
        return False

# --------------------------- ä¸»å…¥å£ ---------------------------

def main():
    warnings.filterwarnings("ignore", category=FutureWarning)

    parser = build_arg_parser()
    args = parser.parse_args()

    # å…ˆå¤„ç† GPU å¼€å…³ä¸å¯è§è®¾å¤‡ï¼Œå†è¿›è¡Œåç«¯ä¼˜åŒ–
    if args.no_gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        print("ğŸš« å·²ç¦ç”¨GPUåŠ é€Ÿ")
    elif args.gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)
        print(f"ğŸ¯ ä½¿ç”¨GPUè®¾å¤‡: {args.gpu_id}")

    setup_gpu_performance(precision="high", mem_fraction=None if args.no_gpu else args.gpu_memory_fraction)
    log_gpu_environment()

    # è‡ªæ£€ï¼ˆå¯é€‰è·³è¿‡ compile æµ‹è¯•ï¼‰
    gpu_ok = True
    if not args.no_gpu:
        gpu_ok = test_gpu_functionality(skip_compile_test=args.skip_compile_test)
        if not gpu_ok:
            print("âš ï¸ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            args.no_gpu = True

    setup_plot_fonts()

    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆè‹¥æä¾›ï¼‰
    config: dict = {}
    if args.config:
        try:
            with open(args.config, "r", encoding="utf-8") as f:
                config = json.load(f)
        except Exception as e:
            print(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ï¼ˆå°†ç»§ç»­ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼‰ï¼š{e}")

    # æ•°æ®åˆå¹¶ï¼šä¼˜å…ˆ data_glob
    actual_data_dir = args.data_dir
    data_glob = args.data_glob or config.get("data_glob")
    merge_strategy = args.merge_strategy or config.get("merge_strategy", "copy")
    if data_glob:
        print(f"ğŸ” é€šè¿‡é€šé…ç¬¦æ”¶é›†æ•°æ®: {data_glob}")
        actual_data_dir = gather_data_to_merged_dir(data_glob, merge_strategy)

    # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
    ok = False
    if args.test_gpu:
        print("\nğŸ§ª ä»…è¿›è¡ŒGPUåŠŸèƒ½æµ‹è¯•...")
        ok = test_gpu_functionality(skip_compile_test=args.skip_compile_test)
    elif args.quick_start or args.feature_only:
        ok = run_feature_extraction_only(actual_data_dir)
    elif args.data_only:
        ok = run_data_labeling_only(actual_data_dir)
    elif args.full_pipeline:
        ok = run_full_pipeline(actual_data_dir, args.models, config, args)
    else:
        print("\nâš ï¸ æœªé€‰æ‹©å…·ä½“æ¨¡å¼ï¼Œå»ºè®®ä½¿ç”¨ --quick-start æˆ– --full-pipelineã€‚")
        parser.print_help()
        return

    if not ok:
        sys.exit(1)

if __name__ == "__main__":
    main()
